Modelle
-----------------------------------------------------------------------------------------------------
- Richtiges Modell zu finden ist nicht immer offensichtlich -> Trial & Error, iterative Verbesserungen
- Modelle unterscheiden sich durch Art der Kommunikation / Art der Arbeitsaufteilung


Boss / Worker Modell:
- Auch Boss / Slave
- Boss-Thread für Aufgabenverteilung verantwortlich
- Boss soll möglichst wenig Arbeit machen (damit er nicht die Worker blockieren kann)
- Worker-Threads für Ausführung verantwortlich
- Boss wartet evtl. auf Erledigung
- Worker-Threads werden "on demand" erstellt
- Code-Beispiel: S.41
- Alternative: Thread-Pools:
    - Definierte Menge von Threads von Anfang an in Pool bereit
    - Arbeitspakete in FIFO, werden nach und nach Threads aus Pool zugeteilt
    - Weniger Overhead mit Erstellen, mehr mit Verwalten
    - Code-Beispiel: S.43
- Anwendungsbereich: Gut geeignet für Server
    - Asynchrone Anfragen/Ereignisse: Boss
    - Erledigung: Worker
    - Z.B. ein Load-Balancer
- Kommunikation zwischen Boss und Worker minimieren
- Abhängigkeit zwischen Workern minimieren
- Beispielcode:
    main {// boss
        for (;;)// unendlicher Loop
        hole aufgabe
        switch aufgabe
            case X: pthread_create task_X// worker 1
            case Y: pthread_create task_Y// worker 2
            ...
    }
- Verbesserung: Thread-Pools
    - Definierte Menge von Threads schon im Voraus erstellen
    - Weniger Overhead mit Threads erstellen, dafür etwas mehr mit Threads verwalten
    -> FIFO


Peer Modell:
- Auch Workcrew
- Kein Chef
- Gleichzeitiges Erledigen der Arbeiten
- Ein Thread muss alle anderen erstellen, restliche Threads sind voneinander unabhängig
- Peers kennen Input im Voraus (Zuständigkeit) und haben eigenen Weg um Input zu bekommen
- Peers teilen Input miteinander
- Code-Beispiel: S.47
- Anwendungsbereich:
    - Fest oder gut definierte Menge von Eingaben
    - Voneinander unabhängige Aufgaben mit wenig Koordination
    - Möglichst wenig geteilte Ressourcen -> Damit Peers nicht zu oft synchronisiert werden müssen
    - z.B: Matrix-Operationen, Primzahlengenerator, Simulationen
- Da kein Boss, muss Zugang zu I/O synchronisiert sein
- Bei viel Synchronisation: Blockaden (langsam) -> Geteilte Ressourcen minimieren


Pipeline Modell:
- Input-Strom, mit jedem Paket müssen einige Schritte durchgeführt werden
- Suboperationen (Filter, Etapen), Einheiten mit eigenem Input
- 1 Thread pro Arbeitsschritt
- 1. Thread erhält Input, bearbeitet, gibt Resultat weiter, ..., letzter Thread stellt Output bereit
- Während Thread 2, das 1. Paket verarbeitet, arbeitet Thread 1 schon am 2. Paket. Thread 3 tut zu diesem Zeitpunkt noch nichts
- Anwendungsbereich / Beispiel:
    - Fliessband in Autofabrik
    - RISC CPU: Operation holen, decodieren, berechnen, speichern
    - Bildverarbeitung, Signalverarbeitung, Textverarbeitung (Unix Pipes), Filtering (Kriterium 1 -> Kriterium 2 -> ...)
    - Kommunikation zwischen Threads: Message-Passing (so funktioniert Erlang, Haskell, ...)
- Wenn Etappen voneinander unabhängig -> Parallele Ausführung der Etappen


- Vorteile:
    - Ist schneller für viele Daten
- Nachteile:
    - Dauert länger für einzelnes Datum
    - System ist so langsam, wie schwächstes Glied -> Lösung: Z.B. mehrere von diesen haben
- Code-Beispiel: Slide 51


Kombination von Modellen:
- Bei Bedarf Kombination sinnvoll
- Pipeline:
    - Ganzes Programm nur so schnell wie langsamste Stufe
    -> Einzelne (langsame) Stufe parallelisieren
    -> Wichtig, dass alle Stufen etwa gleich lang benötigen (Ausbalancierung)
- Bei Peer Modell können einzelne Peers Pipelines verwenden
- ...


Datenbuffering zwischen Threads
-----------------------------------------------------------------------------------------------------
- Ziel: Austausch von Daten zwischen Threads
- Verschiedene Modelle
- Mechanismen: Lock, suspend/resume-Mechanismus
- Aufbau: Producer -> Buffer -> Consumer
- Producer:
    1. Buffer locken
    2. In Buffer schreiben
    3. Buffer unlocken
    4. Notifikation an wartende Konsumenten
- Consumer:
    1. Buffer locken
    2. So lange warten, bis Buffer nicht leer ist (dazwischen immer wieder unlocken!)
    3. Daten aus Buffer lesen
    4. Buffer unlocken


Double Buffering:
- Wenn Threads miteinander Daten tauschen müssen (z.B. bei Peer-Modell)
- Beide Threads sowohl Producer, als auch Consumer


Probleme:
- Anfällig für Bugs -> Konzentration, Geduld
- Probleme meist im Management von geteilten Ressourcen:
    - Locks
    - Race Conditions
    - Deadlocks
    - Ressource Starvation
- Lösungsansatz:
    - Lock vor Zugriff auf geteilte Ressourcen
    - Unlock, sobald Ressource nicht mehr verwendet wird
    -> Nicht so einfach, wenn Locks dynamisch erstellt werden müssen
- Charakteristisch für Multithreading-Fehlern:
    - Nicht korrekte Resultate
    - Datenkorruption
    - Schwierig zu reproduzieren (100x OK, 1x falsch)
    - Teil des Programmes steht:
        - Warten auf Lock, der nicht lossgelassen wird
        - warten auf Bedingung, die nie erreicht wird


Beispiele für MultiThreaded Applikationen:
- Bankomat:
    - Es kann gleichzeitig Geld abgehoben und im Hintergrund Statistik erstellt werden
    - Locking eines Kontos während geschrieben wird
    - Modell: Boss-Worker
- Matrizenmultiplikation:
    - Modell: Peer-Modell
    - Aufteilung in Teilprobleme, parallele Berechnung, am Schluss wieder zusammenfügen

