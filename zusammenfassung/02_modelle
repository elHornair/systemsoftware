Modelle
-----------------------------------------------------------------------------------------------------
- Richtiges Modell zu finden ist nicht immer offensichtlich -> Trial & Error, iterative Verbesserungen
- Modelle unterscheiden sich durch Art der Kommunikation / Art der Arbeitsaufteilung


Boss / Worker Modell:
- Auch Boss / Slave
- Boss-Thread für Aufgabenverteilung verantwortlich
- Worker-Threads für Ausführung verantwortlich
- Boss wartet evtl. auf Erledigung
- Worker-Threads werden "on demand" erstellt
- Code-Beispiel: S.41
- Alternative: Thread-Pools:
    - Definierte Menge von Threads von Anfang an in Pool bereit
    - Arbeitspakete in FIFO, werden nach und nach Threads aus Pool zugeteilt
    - Weniger Overhead mit Erstellen, mehr mit Verwalten
    - Code-Beispiel: S.43
- Anwendungsbereich: Gut geeignet für Server:
    - Asynchrone Anfragen/Ereignisse: Boss
    - Erledigung: Worker
- Kommunikation zwischen Boss und Worker minimieren
- Abhängigkeit zwischen Workern minimieren


Peer Modell:
- Auch Workcrew
- Kein Chef
- Gleichzeitiges Erledigen der Arbeiten
- Ein Thread muss alle anderen erstellen
- Peers kennen Input im Voraus (Zuständigkeit) und haben eigenen Weg um Input zu bekommen
- Peers teilen Input miteinander
- Code-Beispiel: S.47
- Anwendungsbereich:
    - Fest oder gut definierte Menge von Eingaben
    - Voneinander unabhängige Aufgaben mit wenig Koordination
    - z.B: Matrix-Operationen, Primzahlengenerator, Simulationen
- Da kein Boss, muss Zugang zu I/O synchronisiert sein
- Bei viel Synchronisation: Blockaden (langsam) -> Geteilte Ressourcen minimieren


Pipeline Modell:
- Input-Strom
- Suboperationen (Filter, Etapen), Einheiten mit eigenem Input
- 1. Thread erhält Input, bearbeitet, gibt Resultat weiter, ..., letzter Thread stellt Output bereit
- Anwendungsbereich / Beispiel:
    - Fliessband in Autofabrik
    - RISC CPU: Operation holen, decodieren, berechnen, speichern
    - Bildverarbeitung, Signalverarbeitung, Textverarbeitung (Unix Pipes), Filtering
- Wenn Etapen unabhängig -> Parallele Ausführung der Etapen
- Code-Beispiel: S.51


Kombination von Modellen:
- Bei Bedarf Kombination sinnvoll
- Pipeline:
    - Ganzes Programm nur so schnell wie langsamste Stufe
    -> Einzelne (langsame) Stufe parallelisieren
    -> Wichtig, dass alle Stufen etwa gleich lang benötigen (Ausbalancierung)
- Bei Peer Modell können einzelne Peers Pipelines verwenden
- ...


Datenbuffering zwischen Threads
-----------------------------------------------------------------------------------------------------
- Ziel: Austausch von Daten zwischen Threads
- Verschiedene Modelle
- Mechanismen: Lock, suspend/resume-Mechanismus
- Aufbau: Producer -> Buffer -> Consumer
- Producer:
    1. Buffer locken
    2. In Buffer schreiben
    3. Buffer unlocken
    4. Notifikation an wartende Konsumenten
- Consumer:
    1. Buffer locken
    2. So lange warten, bis Buffer nicht leer ist (dazwischen immer wieder unlocken!)
    3. Daten aus Buffer lesen
    4. Buffer unlocken


Double Buffering:
- Wenn Threads miteinander Daten tauschen müssen (z.B. bei Peer-Modell)
- Beide Threads sowohl Producer, als auch Consumer


Probleme:
- Anfällig für Bugs -> Konzentration, Geduld
- Probleme meist im Management von geteilten Ressourcen:
    - Locks
    - Race Conditions
    - Deadlocks
    - Ressource Starvation
- Lösungsansatz:
    - Lock vor Zugriff auf geteilte Ressourcen
    - Unlock, sobald Ressource nicht mehr verwendet wird
    -> Nicht so einfach, wenn Locks dynamisch erstellt werden müssen
- Charakteristisch für Multithreading-Fehlern:
    - Nicht korrekte Resultate
    - Datenkorruption
    - Schwierig zu reproduzieren (100x OK, 1x falsch)
    - Teil des Programmes steht:
        - Warten auf Lock, der nicht lossgelassen wird
        - warten auf Bedingung, die nie erreicht wird


Beispiele für MultiThreaded Applikationen:
- Bankomat:
    - Es kann gleichzeitig Geld abgehoben und im Hintergrund Statistik erstellt werden
    - Locking eines Kontos während geschrieben wird
    - Modell: Boss-Worker
- Matrizenmultiplikation:
    - Modell: Peer-Modell
    - Aufteilung in Teilprobleme, parallele Berechnung, am Schluss wieder zusammenfügen

