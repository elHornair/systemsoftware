Einführung
-----------------------------------------------------------------------------------------------------

Beispiele von Parallelismus in Hardware:
- CPU-Pipelines
- SMP / Multicore

Geschichte Multithreading:
- Threads: Seit ~1950 (Notizen, Folien sagen ca. 25 Jahre)
- Anfänge:
    - Ballistische Berechnunge
    - Echtzeit-Systeme (realtime systems)
    - Buchungssysteme
    - Games
- Timesharing -> Unix mit Prozessen (~1975)
- Am Anfang selbstgemacht
- Dann proprietäre Bibliotheken. Probleme damit:
    - Verschiedene Style
    - Inkompatible Interfaces
    - Keine Interoperabilität / Portabilität
- 80er: Forschungsgegenstand in OS-Forschung
- 90er: Verschiedene Bibliotheken
- 1996: IEEE Posix-Standard inkl. Pthreads-Standard (leichtgewichtig, portabel, einfach zu benutzen)
- Eher neu: Multicore


Grundlagen Threads
-----------------------------------------------------------------------------------------------------

Warum Threads?
- Anfangs nur langsame Computer -> Serielles Arbeiten sinnvoll
- Später: Multiuser Timesharing (Mehrere Benutzer arbeiten am gleichen System)
- CPUs wurden immer schneller und konnten nicht mehr voll ausgelastet werden -> mehrere Prozesse teilen sich einen Core
- Deshalb: Multitasking
- Heutige Computer: Gleichzeitig viele verschiedene Aufgaben
- Threads weil:
    - Bessere Antwortzeiten (fühlt sich besser an -> Kein eingefrohrenen GUI währen Rechenprozess)
    - Bessere Ressourcenausnutzung
    - Früher hat man das mit process forking erreicht, ist aber mit einem gewissen Aufwand verbunden
    - Threads sind effizienter als Prozess-Forking -> Man will Rechenzeit besser ausnutzen
- Threads werden auch LWP (leight weight process) genannt
- Heute werden CPUs kaum mehr schneller -> Deshalb mehr CPUs (Anzahl Cores / Rechner verdoppelt sich ungefähr alle 2 Jahre ("Neues Moore-Law"))


Was sind Threads?
- Semantisch: Voneinander parallel ausgeführte Programmteile innerhalb des gleichen Prozesses
- Syntaktisch: Z.B. mit pthread_create (Thread erstellen mit Pthreads)
- Pthread definiert Thread durch Funktion, die Start und Ende des Threads definiert


Ressourcenaufteilung Thread:
- Prozesseigene Ressourcen: Globale Daten, Instruktionen (Programm), etc.
- Threadeigene Ressourcen: Stack, Program Counter, CPU-Register


Potentieller Parallelismus - Wo bieten sich Threads an?
- Unabhängige Aufgaben:
    - Unterschiedliche Ressourcen
    - Resultat ist unabhängig von Ausführungsreihenfolge der Teilaufgaben
    -> Je mehr Abhängigkeiten, um so mehr geblockte Tasks, die aufeinander warten
- Blockierender / Überlappender I/O:
    - Blockierung: Typisch für I/O
    - Währenddessen andere Aufgabe ausführbar?
    -> I/O-Aufgabe wird von Thread ausgeführt
- Starke CPU-Beanspruchung:
    - z.B: kryptografische Funktionen, Matrizen, Kompression
    - Parallel durchführen, Programm kann mit anderem Thread trotzdem noch auf I/O reagieren
- Asynchrone Ereignisse:
    - z.B: Netzwerk, User-I/O (Tastatur, Maus, ...), Hardware-Interrupts
    - Zufällige Intervalle zwischen Daten I/O
    -> Behandlungsroutinen können in einen Thread gekapselt werden
- Realtime-Scheduling:
    - Unterschiedliche Prioritäten der Aufgaben


Konkurrentes Programmieren:
- Hierzu gehören:
    - Parallele Ausführung
    - Synchronisierung
    - Kommunikation / Datenaustausch
- Reihenfolge ist potentiell unbekannt / irrelevant
- Konkurrent kann auch timeshared sein
- Unter "nacktem" Unix:
    - Forking:
        - Elternteil / Kind
        - Beide haben jeweils eigene PID
        - Command: fork (Return-Wert ist PID oder 0)
    - Vorteile: Speicher- und Ressourcenschutz, sehr einfach
    - Nachteile: Aufwändige Synchronisation, ziemlich unübersichtlich
- Mit Threads:
    - Weniger Programmoverhead
    - Weniger Systemoverhead
    - Keine Eltern:
        - Threads sind gleichwertig
        - Es gibt allerdings noch den "Main-Thread"


Synchronisation:
- Bei seriellen Programmen: Reihenfolge des Codes
- Bei Unix Multiprocessing: waitpid
- Bei Pthreads:
    - pthread_join
    - Mutexe (mutual exclusion) um "ein Schloss um Ressource zu legen"
    - Bedingungsvariablen
    - Weniger Wartezeit, bessere (feinere) Synchronisation


Kommunikation / Datenaustausch:
- Prozesse:
    - Keine Daten werden zwischen Prozessen geteilt
    - IPC (inter process communication):
        - Sockets
        - Shared Memory
        - Message Passing
        -> Jedes Mal OS-Aufruf (teuer!)
- Threads:
    - Alle Daten werden zwischen Threads geteilt
    - Kommunikation über globale Variablen
    - Fehlerpotential, weil alle Daten geteilt werden -> Deshalb: Mutexe


Identität von Threads:
- Wer bin ich?
- Identifikation mittels Pthread-Handle
- Eigene Identität: pthread_self
- Vergleich mit anderer Identität: pthread_equal


Terminierung:
- Prozess:
    - Wird am Ende von "main()" beendet
    - "main_entry" und "main_exit" zu jedem Programm gelinkt. Zuständig für:
        - Rückgabe von Exit-Wert
        - Befreiung von Systemressourcen
- Thread:
    - Wird am Ende der entsprechenden Funktion beendet
    - Alternativ: "pthread_exit" oder "pthread_cancel"
- Exit-Status und Rückgabewert:
    - Pthread kann joinable oder detached (kein Rückgabewert möglich, da keine Referenz mehr) sein
    - int pthread_join(pthread_t th, void **thread_return);
    - Rückgabewert wird geliefert von "pthread_exit" oder von Callback, das "pthread_create" übergeben wurde
    - Callback: Gibt (void *) zurück -> Typecast machen


Bibliotheksaufrufe & Fehler:
- Pthread Bibliotheksaufrufe geben bei Erfolg Null, ansonsten Fehlercode zurück
- Fehlercodes sind in errno.h definiert
    -> Im Code "#include <errno.h>", dann gibts z.B. "EINVAL" (ungültige Argumente)
- Alternativ: Klartext des Fehlers ausgeben: fprintf(stderr, "Fehler: %s\n", strerror(rtn));


Threads vs. Prozesse:
- Prozesse teuerer (Setupzeit, Speicherverbrauch, IPC, Kontextswitches)
- Threads einfacher
- Grosser Teil kann in Userspace erledigt werden (pthreads Bibliothek, Kommunikation, Synchronisation)


Beispiele für Multithreaded Applikationen:
- Server: DB, Fileserver, HTTP, P2P, ...
- Komplexes Rechnen
- Multitasking-Apps


Modelle
-----------------------------------------------------------------------------------------------------
- Richtiges Modell zu finden ist nicht immer offensichtlich -> Trial & Error, iterative Verbesserungen
- Modelle unterscheiden sich durch Art der Kommunikation / Art der Arbeitsaufteilung


Boss / Worker Modell:
- Auch Boss / Slave
- Boss-Thread für Aufgabenverteilung verantwortlich
- Worker-Threads für Ausführung verantwortlich
- Boss wartet evtl. auf Erledigung
- Worker-Threads werden "on demand" erstellt
- Code-Beispiel: S.41
- Alternative: Thread-Pools:
    - Definierte Menge von Threads von Anfang an in Pool bereit
    - Arbeitspakete in FIFO, werden nach und nach Threads aus Pool zugeteilt
    - Weniger Overhead mit Erstellen, mehr mit Verwalten
    - Code-Beispiel: S.43
- Anwendungsbereich: Gut geeignet für Server:
    - Asynchrone Anfragen/Ereignisse: Boss
    - Erledigung: Worker
- Kommunikation zwischen Boss und Worker minimieren
- Abhängigkeit zwischen Workern minimieren


Peer Modell:
- Auch Workcrew
- Kein Chef
- Gleichzeitiges Erledigen der Arbeiten
- Ein Thread muss alle anderen erstellen
- Peers kennen Input im Voraus (Zuständigkeit) und haben eigenen Weg um Input zu bekommen
- Peers teilen Input miteinander
- Code-Beispiel: S.47
- Anwendungsbereich:
    - Fest oder gut definierte Menge von Eingaben
    - Voneinander unabhängige Aufgaben mit wenig Koordination
    - z.B: Matrix-Operationen, Primzahlengenerator, Simulationen
- Da kein Boss, muss Zugang zu I/O synchronisiert sein
- Bei viel Synchronisation: Blockaden (langsam) -> Geteilte Ressourcen minimieren


Pipeline Modell:
- Input-Strom
- Suboperationen (Filter, Etapen), Einheiten mit eigenem Input
- 1. Thread erhält Input, bearbeitet, gibt Resultat weiter, ..., letzter Thread stellt Output bereit
- Anwendungsbereich / Beispiel:
    - Fliessband in Autofabrik
    - RISC CPU: Operation holen, decodieren, berechnen, speichern
    - Bildverarbeitung, Signalverarbeitung, Textverarbeitung (Unix Pipes), Filtering
- Wenn Etapen unabhängig -> Parallele Ausführung der Etapen
- Code-Beispiel: S.51


Kombination von Modellen:
- Bei Bedarf Kombination sinnvoll
- Pipeline:
    - Ganzes Programm nur so schnell wie langsamste Stufe
    -> Einzelne (langsame) Stufe parallelisieren
    -> Wichtig, dass alle Stufen etwa gleich lang benötigen (Ausbalancierung)
- Bei Peer Modell können einzelne Peers Pipelines verwenden
- ...





Stand: S.52/53





Beispiel pthread_create:
-----------------------------------------------------------------------------------------------------
extern int pthread_create(
    pthread_t *thread_handle,
    const pthread_attr_t *thread_attribute,
    void * (*)funktion(void *),
    void *parameter
);






TODO:
- checken, seit wanns threads gibt (in notizen anpassen)
- rest (pthread_create beispiel) noch irgendwo einbauen
- umbenennen in "gundlagen"
- notes aus schule mergen
- Stand Unterricht? S. xx?



stand unterricht: Folie 91
