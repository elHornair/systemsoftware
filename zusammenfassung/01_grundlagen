Einführung
-----------------------------------------------------------------------------------------------------

Beispiele von Parallelismus in Hardware:
- CPU-Pipelines
- SMP / Multicore

Geschichte Multithreading:
- Threads: Seit ~1950 (Notizen, Folien sagen ca. 25 Jahre)
- Anfänge:
    - Ballistische Berechnunge
    - Echtzeit-Systeme (realtime systems)
    - Buchungssysteme
    - Games
- Timesharing -> Unix mit Prozessen (~1975)
- Am Anfang selbstgemacht
- Dann proprietäre Bibliotheken. Probleme damit:
    - Verschiedene Style
    - Inkompatible Interfaces
    - Keine Interoperabilität / Portabilität
- 80er: Forschungsgegenstand in OS-Forschung
- 90er: Verschiedene Bibliotheken
- 1996: IEEE Posix-Standard inkl. Pthreads-Standard (leichtgewichtig, portabel, einfach zu benutzen)
- Eher neu: Multicore


Grundlagen Threads
-----------------------------------------------------------------------------------------------------

Warum Threads?
- Anfangs nur langsame Computer -> Serielles Arbeiten sinnvoll
- Später: Multiuser Timesharing (Mehrere Benutzer arbeiten am gleichen System)
- CPUs wurden immer schneller und konnten nicht mehr voll ausgelastet werden -> mehrere Prozesse teilen sich einen Core
- Deshalb: Multitasking
- Heutige Computer: Gleichzeitig viele verschiedene Aufgaben
- Threads weil:
    - Bessere Antwortzeiten (fühlt sich besser an -> Kein eingefrohrenen GUI währen Rechenprozess)
    - Bessere Ressourcenausnutzung
    - Früher hat man das mit process forking erreicht, ist aber mit einem gewissen Aufwand verbunden
    - Threads sind effizienter als Prozess-Forking -> Man will Rechenzeit besser ausnutzen
- Threads werden auch LWP (leight weight process) genannt
- Heute werden CPUs kaum mehr schneller -> Deshalb mehr CPUs (Anzahl Cores / Rechner verdoppelt sich ungefähr alle 2 Jahre ("Neues Moore-Law"))


Was sind Threads?
- Semantisch: Voneinander parallel ausgeführte Programmteile innerhalb des gleichen Prozesses
- Syntaktisch: Z.B. mit pthread_create (Thread erstellen mit Pthreads)
- Es gibt Threads auch auf höheren Ebenen: Java, Ruby, ...
- Linux per se hat keine Threads, diese kommen erst mit libc


Ressourcenaufteilung Thread:
- Prozesseigene Ressourcen: Globale Daten, Instruktionen (Programm), etc.
- Threadeigene Ressourcen: Stack, Program Counter, CPU-Register
- Speicheraufteilung Thread:
    - Stack mit stack frames (lokale Variablen, Rücksprungadresse, ...)
    - Globale Daten
    - Code (program counter zeigt irgendwo hierauf)
    - Heap
    -> Stack baut sich auf (mit jedem Verschachtelungslevel)


Parallelismus / Potentieller Parallelismus - Wo bieten sich Threads an?
- Pseudo-Parallelität: Timesharing (mehrere Prozesse teilen einen Core)
- Echte Parallelität bringt neues Problem mit sich:
    - Gewisse Teilaufgaben hangen voneinander ab
    -> D.h. man kann nur voneinander unabhängige Aufgaben parallelisieren
- Allgemein: Unabhängige Aufgaben:
    - Unterschiedliche Ressourcen
    - Resultat ist unabhängig von Ausführungsreihenfolge der Teilaufgaben
    -> Je mehr Abhängigkeiten, um so mehr geblockte Tasks, die aufeinander warten
- Blockierender / Überlappender I/O:
    - Blockierung: Typisch für I/O
    - Währenddessen andere Aufgabe ausführbar?
    -> I/O-Aufgabe wird von Thread ausgeführt
- Starke CPU-Beanspruchung:
    - z.B: kryptografische Funktionen, Matrizen, Kompression
    - Parallel durchführen, Programm kann mit anderem Thread trotzdem noch auf I/O reagieren
- Asynchrone Ereignisse:
    - z.B: Netzwerk, User-I/O (Tastatur, Maus, ...), Hardware-Interrupts
    - Zufällige Intervalle zwischen Daten I/O
    -> Behandlungsroutinen können in einen Thread gekapselt werden
- I/O, respektive blockierendes I/O -> früher wurde CPU während I/O nicht benutzt
    -> Deshalb: Maschinenbefehle aufgeteilt, so dass während verarbeiten von Datum schon wieder nächstes gelesen wird
- Realtime:
    - z.B. Videostreaming, Telefonie, ...


Paralleles / Konkurrentes Programmieren:
- Hierzu gehören:
    - Parallele Ausführung (kann auch implizit sein):
        - z.B. bei Erlang (alles parallel)
        - XSLT (Engine parallelisiert selbst, man deklariert nur)
        - CSS (browser kümmert sich drum)
    - Synchronisierung:
        - Muss nicht zwingend benutzt werden
        - Wenn ich z.B. nur eine Message broadcasten will, brauche ich keine Synchronisation
    - Kommunikation / Datenaustausch
- Reihenfolge ist potentiell unbekannt / irrelevant
- Konkurrent kann auch timeshared sein
- Unter "nacktem" Unix mit Forking:
    - Vergabelung
    - Prozess wird kopiert (Elternteil / Kind)
    - Die beiden Prozesse sind voneinander unabhängig, es ist für A nicht möglich, auf die Daten von B zuzugreifen
    - Beide haben jeweils eigene PID
    - Interprozesskommunikation: "Everything's a file" -> A schreibt Daten in File, B liest sie
    - Child hat auf gleiche Ressourcen Zugriff, wie Parent (gleiche Rechte, etc.)
    - Command: fork (Return-Wert ist PID oder 0)
    - Fork gibt zurück:
        - 0 für Kind
        - PID des für Parent-Prozess
        -> Dies wird verwendet, um die Prozesse im Folgenden voneinander zu unterscheiden (Parent oder Child?)
    - Vorteile:
        - Speicher / Ressourcenschutz
        - Sehr einfach
    - Nachteile
        - Speicherschutz (weil manchmal möchte man ja Daten teilen)
        - Synchronisation ist aufwändiger
        - Unübersichtlich
- Mit Threads:
    - Weniger Programmoverhead
    - Weniger Systemoverhead
    - Keine Eltern:
        - Threads sind gleichwertig/gleichberechtigt
        - Es gibt allerdings noch den "Main-Thread"


Synchronisation:
- Bei seriellen Programmen: Reihenfolge des Codes
- Bei Unix Multiprocessing: waitpid
- Bei Pthreads:
    - pthread_join
    - Mutexe (mutual exclusion) um "ein Schloss um Ressource zu legen"
    - Bedingungsvariablen
    - Weniger Wartezeit, bessere (feinere) Synchronisation


Kommunikation / Datenaustausch:
- Prozesse:
    - Keine Daten werden zwischen Prozessen geteilt
    - IPC (inter process communication):
        - Sockets
        - Shared Memory
        - Message Passing
        -> Jedes Mal OS-Aufruf (teuer!)
- Threads:
    - Alle Daten werden zwischen Threads geteilt
    - Kommunikation über globale Variablen
    - Fehlerpotential, weil alle Daten geteilt werden -> Deshalb: Mutexe


Identität von Threads:
- Wer bin ich?
- Identifikation mittels Pthread-Handle
- Eigene Identität: pthread_self
- Vergleich mit anderer Identität: pthread_equal


Terminierung:
- Prozess:
    - Wird am Ende von "main()" beendet
    - "main_entry" und "main_exit" zu jedem Programm gelinkt. Zuständig für:
        - Rückgabe von Exit-Wert
        - Befreiung von Systemressourcen
- Thread:
    - Wird am Ende der entsprechenden Funktion beendet
    - Alternativ: "pthread_exit" oder "pthread_cancel"
- Exit-Status und Rückgabewert:
    - Pthread kann joinable oder detached (kein Rückgabewert möglich, da keine Referenz mehr) sein
    - int pthread_join(pthread_t th, void **thread_return);
    - Rückgabewert wird geliefert von "pthread_exit" oder von Callback, das "pthread_create" übergeben wurde
    - Callback: Gibt (void *) zurück -> Typecast machen


Bibliotheksaufrufe & Fehler:
- Pthread Bibliotheksaufrufe geben bei Erfolg Null, ansonsten Fehlercode zurück
- Fehlercodes sind in errno.h definiert
    -> Im Code "#include <errno.h>", dann gibts z.B. "EINVAL" (ungültige Argumente)
- Alternativ: Klartext des Fehlers ausgeben: fprintf(stderr, "Fehler: %s\n", strerror(rtn));


Threads vs. Prozesse:
- Prozesse teuerer (Setupzeit, Speicherverbrauch, IPC, Kontextswitches)
- Threads einfacher
- Grosser Teil kann in Userspace erledigt werden (pthreads Bibliothek, Kommunikation, Synchronisation)


Beispiele für Multithreaded Applikationen:
- Server: DB, Fileserver, HTTP, P2P, ...
- Komplexes Rechnen
- Multitasking-Apps


Pthreads (highlevel)
-----------------------------------------------------------------------------------------------------
- Definiert Thread durch Funktion, die Start und Ende des Threads definiert
- Syntaxdefinition (Standard) ist so alt wie Pthreads selbst
- Fun fact: Semantik war bis vor ca. 2 Jahren nicht als Standard definiert
- Vorteile:
    - Einfach zu benutzen (im historischen Kontext, heute im Verhältnis eher schwierig)
    - Lightweight (stimmt auch heute noch)
    - Portabel (stimmt auch heute noch, abgesehen von Windows -> Dafür gibts aber weitere Abstraktion: ApacheRuntime)
    - Java-Syntaktisch
- Anwendungen:
    - Ada, Concurrent Pascal
    - Closure, Erlang
        - Warum gibts Closure / Erlang?
        - Um einfach parallele Programme schreiben zu können
        - Aber anderer Lösungsansatz: In Closure/Erlang gibts keinen shared state
- Beispiel:
    int pthread_create(
        pthread_t *thread,                  <----- Identifier (ID) (wird von pthread_create in pthread_t reingeschrieben)
        const pthread_attr_t *attr,         <----- Attribute, die Thread konfigurieren
        void *(*start_routine)(void*),      <----- Funktion, die von Thread ausgeführt wird (Void-Pointer: Argument und Rückgabewert sind beliebig)
        void *arg);                         <----- Parameter, die Thread übergeben werden


Some things about C
-----------------------------------------------------------------------------------------------------
- Parameterübergabe ist immer "by value" (auch für komplexe Datentypen wie struct), ausser es wird explizit ein Pointer übergeben

Pointer:
- Referenzierungsoperator: myPointer = &myVar (gibt den Pointer einer Variable zurück, also eine Referenz der Variable)
- Dereferenzierungsoperator: myVar = *myPointer (gibt die Variable, auf die der Pointer zeigt zurück)
- Beispiel: int* a = &A; (dem int-pointer a wird der Pointer zugewiesen, der auf die Variable A zeigt)
