Einführung
-----------------------------------------------------------------------------------------------------

Beispiele von Parallelismus in Hardware:
- CPU-Pipelines
- SMP / Multicore

Geschichte Multithreading:
- Threads: Seit ~1950 (Notizen, Folien sagen ca. 25 Jahre)
- Anfänge:
    - Ballistische Berechnunge
    - Echtzeit-Systeme (realtime systems)
    - Buchungssysteme
    - Games
- Timesharing -> Unix mit Prozessen (~1975)
- Am Anfang selbstgemacht
- Dann proprietäre Bibliotheken. Probleme damit:
    - Verschiedene Style
    - Inkompatible Interfaces
    - Keine Interoperabilität / Portabilität
- 80er: Forschungsgegenstand in OS-Forschung
- 90er: Verschiedene Bibliotheken
- 1996: IEEE Posix-Standard inkl. Pthreads-Standard (leichtgewichtig, portabel, einfach zu benutzen)
- Eher neu: Multicore


Grundlagen Threads
-----------------------------------------------------------------------------------------------------

Warum Threads?
- Anfangs nur langsame Computer -> Serielles Arbeiten sinnvoll
- Später: Multiuser Timesharing (Mehrere Benutzer arbeiten am gleichen System)
- CPUs wurden immer schneller und konnten nicht mehr voll ausgelastet werden -> mehrere Prozesse teilen sich einen Core
- Deshalb: Multitasking
- Heutige Computer: Gleichzeitig viele verschiedene Aufgaben
- Threads weil:
    - Bessere Antwortzeiten (fühlt sich besser an -> Kein eingefrohrenen GUI währen Rechenprozess)
    - Bessere Ressourcenausnutzung
    - Früher hat man das mit process forking erreicht, ist aber mit einem gewissen Aufwand verbunden
    - Threads sind effizienter als Prozess-Forking -> Man will Rechenzeit besser ausnutzen
- Threads werden auch LWP (leight weight process) genannt
- Heute werden CPUs kaum mehr schneller -> Deshalb mehr CPUs (Anzahl Cores / Rechner verdoppelt sich ungefähr alle 2 Jahre ("Neues Moore-Law"))


Was sind Threads?
- Semantisch: Voneinander parallel ausgeführte Programmteile innerhalb des gleichen Prozesses
- Syntaktisch: Z.B. mit pthread_create (Thread erstellen mit Pthreads)
- Pthread definiert Thread durch Funktion, die Start und Ende des Threads definiert


Ressourcenaufteilung Thread:
- Prozesseigene Ressourcen: Globale Daten, Instruktionen (Programm), etc.
- Threadeigene Ressourcen: Stack, Program Counter, CPU-Register
- Speicheraufteilung Thread:
    - Stack mit stack frames (lokale Variablen, Rücksprungadresse, ...)
    - Globale Daten
    - Code (program counter zeigt irgendwo hierauf)
    - Heap
    -> Stack baut sich auf (mit jedem Verschachtelungslevel)


Potentieller Parallelismus - Wo bieten sich Threads an?
- Unabhängige Aufgaben:
    - Unterschiedliche Ressourcen
    - Resultat ist unabhängig von Ausführungsreihenfolge der Teilaufgaben
    -> Je mehr Abhängigkeiten, um so mehr geblockte Tasks, die aufeinander warten
- Blockierender / Überlappender I/O:
    - Blockierung: Typisch für I/O
    - Währenddessen andere Aufgabe ausführbar?
    -> I/O-Aufgabe wird von Thread ausgeführt
- Starke CPU-Beanspruchung:
    - z.B: kryptografische Funktionen, Matrizen, Kompression
    - Parallel durchführen, Programm kann mit anderem Thread trotzdem noch auf I/O reagieren
- Asynchrone Ereignisse:
    - z.B: Netzwerk, User-I/O (Tastatur, Maus, ...), Hardware-Interrupts
    - Zufällige Intervalle zwischen Daten I/O
    -> Behandlungsroutinen können in einen Thread gekapselt werden
- Realtime-Scheduling:
    - Unterschiedliche Prioritäten der Aufgaben


Konkurrentes Programmieren:
- Hierzu gehören:
    - Parallele Ausführung
    - Synchronisierung
    - Kommunikation / Datenaustausch
- Reihenfolge ist potentiell unbekannt / irrelevant
- Konkurrent kann auch timeshared sein
- Unter "nacktem" Unix:
    - Forking:
        - Elternteil / Kind
        - Beide haben jeweils eigene PID
        - Command: fork (Return-Wert ist PID oder 0)
    - Vorteile: Speicher- und Ressourcenschutz, sehr einfach
    - Nachteile: Aufwändige Synchronisation, ziemlich unübersichtlich
- Mit Threads:
    - Weniger Programmoverhead
    - Weniger Systemoverhead
    - Keine Eltern:
        - Threads sind gleichwertig
        - Es gibt allerdings noch den "Main-Thread"


Synchronisation:
- Bei seriellen Programmen: Reihenfolge des Codes
- Bei Unix Multiprocessing: waitpid
- Bei Pthreads:
    - pthread_join
    - Mutexe (mutual exclusion) um "ein Schloss um Ressource zu legen"
    - Bedingungsvariablen
    - Weniger Wartezeit, bessere (feinere) Synchronisation


Kommunikation / Datenaustausch:
- Prozesse:
    - Keine Daten werden zwischen Prozessen geteilt
    - IPC (inter process communication):
        - Sockets
        - Shared Memory
        - Message Passing
        -> Jedes Mal OS-Aufruf (teuer!)
- Threads:
    - Alle Daten werden zwischen Threads geteilt
    - Kommunikation über globale Variablen
    - Fehlerpotential, weil alle Daten geteilt werden -> Deshalb: Mutexe


Identität von Threads:
- Wer bin ich?
- Identifikation mittels Pthread-Handle
- Eigene Identität: pthread_self
- Vergleich mit anderer Identität: pthread_equal


Terminierung:
- Prozess:
    - Wird am Ende von "main()" beendet
    - "main_entry" und "main_exit" zu jedem Programm gelinkt. Zuständig für:
        - Rückgabe von Exit-Wert
        - Befreiung von Systemressourcen
- Thread:
    - Wird am Ende der entsprechenden Funktion beendet
    - Alternativ: "pthread_exit" oder "pthread_cancel"
- Exit-Status und Rückgabewert:
    - Pthread kann joinable oder detached (kein Rückgabewert möglich, da keine Referenz mehr) sein
    - int pthread_join(pthread_t th, void **thread_return);
    - Rückgabewert wird geliefert von "pthread_exit" oder von Callback, das "pthread_create" übergeben wurde
    - Callback: Gibt (void *) zurück -> Typecast machen


Bibliotheksaufrufe & Fehler:
- Pthread Bibliotheksaufrufe geben bei Erfolg Null, ansonsten Fehlercode zurück
- Fehlercodes sind in errno.h definiert
    -> Im Code "#include <errno.h>", dann gibts z.B. "EINVAL" (ungültige Argumente)
- Alternativ: Klartext des Fehlers ausgeben: fprintf(stderr, "Fehler: %s\n", strerror(rtn));


Threads vs. Prozesse:
- Prozesse teuerer (Setupzeit, Speicherverbrauch, IPC, Kontextswitches)
- Threads einfacher
- Grosser Teil kann in Userspace erledigt werden (pthreads Bibliothek, Kommunikation, Synchronisation)


Beispiele für Multithreaded Applikationen:
- Server: DB, Fileserver, HTTP, P2P, ...
- Komplexes Rechnen
- Multitasking-Apps


Pthreads (highlevel)
-----------------------------------------------------------------------------------------------------
- Einfach zu benutzen (im historischen Kontext, heute im Verhältnis eher schwierig)
- Lightweight (stimmt auch heute noch)
- Portabel (stimmt auch heute noch, abgesehen von Windows -> Dafür gibts aber weitere Abstraktion: ApacheRuntime)
- Java-Syntaktisch
- Anwendungen:
    - Ada, Concurrent Pascal
    - Closure, Erlang
        - Warum gibts Closure / Erlang?
        - Um einfach parallele Programme schreiben zu können
        - Aber anderer Lösungsansatz: In Closure/Erlang gibts keinen shared state
